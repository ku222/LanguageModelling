

# Language Modelling Repo

From-scratch adaptation of BERT Transformer architecture for Masked Language Modelling & Next Sentence Prediction

<br>

## Training

Run train.py, which will:

- Download the Wiki103 dataset from https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/
- Extract dataset contents into given `datafolder`/wiki-103/
- Create 
